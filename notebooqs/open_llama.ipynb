{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Q: What is the largest animal?\n",
      "A: The largest animal is the blue whale.\n",
      "Q: What is the smallest animal?\n",
      "A: The smallest animal is the bee.\n",
      "Q: What is the fastest animal?\n",
      "A: The fastest animal is the cheetah.\n",
      "Q: What is the slowest animal?\n",
      "A: The slowest animal is the sloth.\n",
      "Q: What is the oldest animal?\n",
      "A: The oldest animal is the tortoise.\n",
      "Q: What is the youngest animal?\n",
      "A: The youngest animal is the baby.\n",
      "Q: What is the most expensive animal?\n",
      "A: The most expensive animal is the elephant.\n",
      "Q: What is the most common animal?\n",
      "A: The most common animal is the dog.\n",
      "Q: What is the most dangerous animal?\n",
      "A: The most dangerous animal is the lion.\n",
      "Q: What is the most famous animal?\n",
      "A: The most famous animal is the lion.\n",
      "Q: What is the most famous animal in the world?\n",
      "A: The most famous animal in the world is the lion.\n",
      "Q: What is the most famous animal in the United States?\n",
      "A: The most famous animal in the United States is the lion.\n",
      "Q: What is the most famous animal in the world?\n",
      "A: The most famous animal in the world is the lion.\n",
      "Q: What is the most famous animal in the world?\n",
      "A: The most famous animal in the world is the lion.\n",
      "Q: What is the most famous animal in the world?\n",
      "A: The\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# model_path = 'openlm-research/open_llama_3b'\n",
    "model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lumi/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The following is a conversation with Joe Rogan and the young navy seal Lumi Wolf.\n",
      "Joe Rogan: I’m Joe Rogan, and this is JRE MMA Show #117 with Lumi Wolf.\n",
      "Lumi Wolf: I’m Lumi Wolf, and I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal?\n",
      "Lumi Wolf: Yes, sir.\n",
      "Joe Rogan: How long have you been a Navy Seal?\n",
      "Lumi Wolf: I’ve been a Navy Seal for 10 years.\n",
      "Joe Rogan: 10 years.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’m a Navy Seal. I’\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = 'The following is a conversation with Joe Rogan and the young navy seal Lumi Wolf. '\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=320, top_p=0.9, top_k=0\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-cloud-5WWy5Vs_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
