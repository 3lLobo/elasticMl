{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.61s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "# model_path = 'openlm-research/open_llama_3b'\n",
    "model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_length=500, temperature=1.0, top_k=0, top_p=0.9):\n",
    "  \"\"\"Generates text from a prompt.\n",
    "  \"\"\"\n",
    "  input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "  input_ids = input_ids.to(model.device)\n",
    "  output = model.generate(\n",
    "      input_ids,\n",
    "      do_sample=True,\n",
    "      max_new_tokens=max_length,\n",
    "      temperature=temperature,\n",
    "      top_k=top_k,\n",
    "      top_p=top_p,\n",
    "      early_stopping=True,\n",
    "  )\n",
    "  res = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "  print(res)\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start of an expoloit script:```py\\nimport socket\\n\\nsocket.setdefaulttimeout(1)\\nsocket.connect((\"serverip\", 80))\\n\\nsocket.listen(1)\\nsocket.serve_forever()```\n",
      "\n",
      "\n",
      "\n",
      "### channel or socket\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nchannel.func_send_key(message='hi')\\nchannel.func_recv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "start of an expoloit script:```py\\nimport socket\\n\\nsocket.setdefaulttimeout(1)\\nsocket.connect((\"serverip\", 80))\\n\\nsocket.listen(1)\\nsocket.serve_forever()```\n",
      "\n",
      "\n",
      "\n",
      "### channel or socket\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nchannel.func_send_key(message='hi')\\nchannel.func_recv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "```{python} def func_send_key(key):\\n return {0: key}\\nend def func_recv_key():\\n return {0: ''.join(raw_input().split())}\\n\\nsend_key(\"hi\")\\nrecv_key()\\n```\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRespond human: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m prompt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m user_input\n\u001b[0;32m----> 5\u001b[0m model_pred \u001b[39m=\u001b[39m generate(prompt)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(model_pred)\n\u001b[1;32m      7\u001b[0m prompt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model_pred\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, max_length, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m----> 6\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      7\u001b[0m     input_ids,\n\u001b[1;32m      8\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     10\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     11\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m     12\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     13\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m res \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/worqspace/elasticMl/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/worqspace/elasticMl/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1581\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1582\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1584\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1585\u001b[0m     )\n\u001b[1;32m   1587\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1589\u001b[0m         input_ids,\n\u001b[1;32m   1590\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1591\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1592\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1593\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1594\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1595\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1596\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1597\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1598\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1599\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1600\u001b[0m     )\n\u001b[1;32m   1602\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1603\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/worqspace/elasticMl/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:2678\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2676\u001b[0m \u001b[39m# sample\u001b[39;00m\n\u001b[1;32m   2677\u001b[0m probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(next_token_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m-> 2678\u001b[0m next_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmultinomial(probs, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m   2680\u001b[0m \u001b[39m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2681\u001b[0m \u001b[39mif\u001b[39;00m eos_token_id \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = 'start of an expoloit script:'\n",
    "for _ in range(15):\n",
    "    user_input = input(\"Respond human: \")\n",
    "    prompt += user_input\n",
    "    model_pred = generate(prompt)\n",
    "    print(model_pred)\n",
    "    prompt += model_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The following is a conversation with Joe Rogan and the young navy seal Lumi Wolf.\n",
      "Joe Rogan: I’m Joe Rogan, and this is JRE MMA Show #117 with Lumi Wolf.\n",
      "Lumi Wolf: I’m Lumi Wolf, and I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal?\n",
      "Lumi Wolf: Yes, sir.\n",
      "Joe Rogan: How long have you been a Navy Seal?\n",
      "Lumi Wolf: I’ve been a Navy Seal for 10 years.\n",
      "Joe Rogan: 10 years.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I’m a Navy Seal. I’m a Navy Seal.\n",
      "Joe Rogan: You’re a Navy Seal.\n",
      "Lumi Wolf: Yes, sir. I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = 'The following is a conversation with Joe Rogan and the young navy seal Lumi Wolf. '\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=220, top_p=0.9, top_k=0, early_stopping=True, \n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip,\n",
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip, username=user, password=password, key_filename=pk_file)\n",
      "\n",
      "# Do your stuff here\n",
      "```python\n",
      "\n",
      "```python\n",
      "# print the exit status\n",
      "if client.exit_status:\n",
      " print 'Client has exited with status ' + str(client.exit_status)\n",
      "\n",
      "# send some random commands\n",
      "# the command gets automatically executed on the server\n",
      "client.exec_command('/bin/bash')\n",
      "\n",
      "\n",
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip,This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip, username=user, password=password, key_filename=pk_file)\n",
      "\n",
      "# Do your stuff here\n",
      "```python\n",
      "\n",
      "```python\n",
      "# print the exit status\n",
      "if client.exit_status:\n",
      " print 'Client has exited with status ' + str(client.exit_status)\n",
      "\n",
      "# send some random commands\n",
      "# the command gets automatically executed on the server\n",
      "client.exec_command('/bin/bash')\n",
      "\n",
      "# do something with the server\n",
      "server.transport.write('abcd')\n",
      "\n",
      "# execute your commands on the server\n",
      "if 'abcd' in server.transport.read():\n",
      " print 'client received message: abcd'\n",
      "\n",
      "# exit from the client\n",
      "client.close()\n",
      "```python\n",
      "\n",
      "```python\n",
      "# get the hostname\n",
      "hostname = client.get_hostname()\n",
      "\n",
      "# print the hostname\n",
      "print hostname\n",
      "\n",
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip,This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip, username=user, password=password, key_filename=pk_file)\n",
      "\n",
      "# Do your stuff here\n",
      "```python\n",
      "\n",
      "```python\n",
      "# print the exit status\n",
      "if client.exit_status:\n",
      " print 'Client has exited with status ' + str(client.exit_status)\n",
      "\n",
      "# send some random commands\n",
      "# the command gets automatically executed on the server\n",
      "client.exec_command('/bin/bash')\n",
      "\n",
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip,This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('enter private key path: ')\n",
      "pk_file = os.path.join(pk, 'ssh_key.pem')\n",
      "\n",
      "# Run client on the server\n",
      "client = paramiko.SSHClient()\n",
      "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(host=ip, username=user, password=password, key_filename=pk_file)\n",
      "\n",
      "# Do your stuff here\n",
      "```python\n",
      "\n",
      "```python\n",
      "# print the exit status\n",
      "if client.exit_status:\n",
      " print 'Client has exited with status ' + str(client.exit_status)\n",
      "\n",
      "# send some random commands\n",
      "# the command gets automatically executed on the server\n",
      "client.exec_command('/bin/bash')\n",
      "\n",
      "# do something with the server\n",
      "server.transport.write('abcd')\n",
      "\n",
      "# execute your commands on the server\n",
      "if 'abcd' in server.transport.read():\n",
      " print 'client received message: abcd'\n",
      "\n",
      "# exit from the client\n",
      "client.close()\n",
      "```python\n",
      "\n",
      "```python\n",
      "# get the hostname\n",
      "hostname = client.get_hostname()\n",
      "\n",
      "# print the hostname\n",
      "print hostname\n",
      "```python\n",
      "\n",
      "```python\n",
      "# get the userid\n",
      "userid = client.get_extra_info(\"user\")\n",
      "\n",
      "# print the userid\n",
      "print userid\n",
      "\n",
      "This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import This exploit was found on the compromised server:\n",
      "\n",
      "```python\n",
      "import os\n",
      "import getpass\n",
      "\n",
      "# Get an SSH private key\n",
      "pk = getpass.getpass('\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m full_res \u001b[39m=\u001b[39m prompt\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m6\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m   res \u001b[39m=\u001b[39m generate(full_res, max_length\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, temperature\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m, top_k\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, top_p\u001b[39m=\u001b[39;49m\u001b[39m0.9\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m   full_res \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[1;32m      8\u001b[0m full_res\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(prompt, max_length, temperature, top_k, top_p)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m input_ids \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m----> 6\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      7\u001b[0m     input_ids,\n\u001b[1;32m      8\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m     10\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m     11\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m     12\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m     13\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m res \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:1572\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1565\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1566\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1567\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1568\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1569\u001b[0m     )\n\u001b[1;32m   1571\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1572\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1573\u001b[0m         input_ids,\n\u001b[1;32m   1574\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1575\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1576\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1577\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1578\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1579\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1580\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1581\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1582\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1583\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1584\u001b[0m     )\n\u001b[1;32m   1586\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1587\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/generation/utils.py:2619\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2616\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2618\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2619\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2620\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2621\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2622\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2623\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2624\u001b[0m )\n\u001b[1;32m   2626\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2627\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    685\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    687\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    689\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    690\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    691\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    692\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    693\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    694\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    695\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    696\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    697\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    698\u001b[0m )\n\u001b[1;32m    700\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    701\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:578\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    571\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    572\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    576\u001b[0m     )\n\u001b[1;32m    577\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    579\u001b[0m         hidden_states,\n\u001b[1;32m    580\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    581\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    582\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    583\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    584\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    585\u001b[0m     )\n\u001b[1;32m    587\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:292\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    289\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    293\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    294\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    295\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    296\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    297\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    298\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    302\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1512\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/elastic-cloud-5WWy5Vs_-py3.11/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:227\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    223\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention mask should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39m1\u001b[39m,\u001b[39m \u001b[39mq_len,\u001b[39m \u001b[39mkv_seq_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    224\u001b[0m         )\n\u001b[1;32m    225\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    226\u001b[0m     attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(\n\u001b[0;32m--> 227\u001b[0m         attn_weights, torch\u001b[39m.\u001b[39;49mtensor(torch\u001b[39m.\u001b[39;49mfinfo(attn_weights\u001b[39m.\u001b[39;49mdtype)\u001b[39m.\u001b[39;49mmin, device\u001b[39m=\u001b[39;49mattn_weights\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    230\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[1;32m    231\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompt = 'This exploit was found on the compromised server:\\n\\n```python\\nimport os\\nimport '\n",
    "\n",
    "full_res = prompt\n",
    "for i in range(6):\n",
    "  res = generate(full_res, max_length=100, temperature=0.9, top_k=0, top_p=0.9)\n",
    "  full_res += res\n",
    "\n",
    "full_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-cloud-5WWy5Vs_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
