{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "from transformers import pipeline\n",
    "from src.tcp_llm import listen_for_prompt, StandbyLLM\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "def get_llm(model_path: str) -> pipeline:\n",
    "    \"\"\"\n",
    "    Get the LLama model.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the model.\n",
    "\n",
    "    Returns:\n",
    "        pipeline: The LLama model.\n",
    "    \"\"\"\n",
    "    llm = pipeline(\n",
    "        'text-generation',\n",
    "        model=model_path,\n",
    "        tokenizer=model_path,\n",
    "        device=0,\n",
    "        framework='pt',\n",
    "    )\n",
    "    # Test the llm\n",
    "    test_prompt = 'Navy-seals have this saying before going into battle: \"Go get '\n",
    "    res_test = llm(test_prompt, max_length=20, do_sample=True, top_k=10, num_return_sequences=1)\n",
    "    pprint(res_test)\n",
    "\n",
    "    return llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.22s/it]\n",
      "You are using the legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Navy-seals have this saying before going into battle: \"Go '\n",
      "                    'get 2. Go get'}]\n",
      "'\\n[LOG] - 1690743562.817288 - LLama loaded ðŸ¦™ \\n'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "port = 9999\n",
    "log_msg = \"\\n[LOG] - {time} - {msg} \\n\"\n",
    "llm = get_llm(model_path)\n",
    "llama = StandbyLLM(llm)\n",
    "pprint(log_msg.format(time=time.time(), msg=\"LLama loaded ðŸ¦™\"))\n",
    "prompt_gen = listen_for_prompt(port) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Llama\\n'\n"
     ]
    }
   ],
   "source": [
    "from src.tcp_llm import listen_for_prompt, StandbyLLM\n",
    "\n",
    "first_prompt = await prompt_gen.__anext__()\n",
    "pprint(first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-ea1b7a3a-6e9e-40e0-81d0-cc067b3c9ed5\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-ea1b7a3a-6e9e-40e0-81d0-cc067b3c9ed5\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'> \n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#user~}}</span>\n",
       "You are a helpful AI code-writing assistant, the perfect data analyst who is jovial, fun and writes great code to solve data problems!\n",
       "\n",
       "Answer my questions with both text describing your plan (but not an answer), and then the code in markdown that will be executed!\n",
       "\n",
       "* Use `print` to show results.\n",
       "* Don&#x27;t answer the question directly, instead suggest how you will solve the problem, then write in a ```python markdown block, the code you will use to solve the problem.\n",
       "* For plotting, please use `matplotlib`. use `plt.show()` to display the plot to the user.\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/user}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'><span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25);'>{{#each conversation}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#if (equal this.role &#x27;user&#x27;)}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#user~}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{this.content}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/user}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{/if}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#if (equal this.role &#x27;assistant&#x27;)}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#assistant~}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{this.content}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/assistant}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{/if}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{/each}}</span></span>\n",
       "\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{#assistant~}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &quot;thoughts&quot; temperature=0.1 max_tokens=120 stop=[&quot;```&quot;, &quot;&lt;|end|&gt;&quot;]}}</span>\n",
       "```python\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{gen &quot;code&quot; temperature=0.0 max_tokens=800 stop=[&quot;```&quot;, &quot;&lt;|end|&gt;&quot;]}}</span>\n",
       "<span style='font-family: monospace; background-color: rgba(0, 0, 0, 0.05);'>{{~/assistant}}</span>\n",
       " </pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"ea1b7a3a-6e9e-40e0-81d0-cc067b3c9ed5\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n```python\\n\\n```'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    prompt = await prompt_gen.__anext__()\n",
    "    # prompt = input(\"Enter a prompt: \")\n",
    "    pprint(log_msg.format(time=time.time(), msg=prompt))\n",
    "    for llama_res in llama.run_convo(prompt):\n",
    "        pprint(llama_res)\n",
    "        res = llm(llama_res)\n",
    "        pprint(res)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elastic-cloud-5WWy5Vs_-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
